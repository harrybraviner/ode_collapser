{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5279386-3cff-4c20-9cf4-bb9d02d2aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c6e55b-36bf-4088-bcd7-4301f17d2ea4",
   "metadata": {},
   "source": [
    "# Notation\n",
    "\n",
    "This notebook solves $\\frac{\\mathrm{d}^2 x}{\\mathrm{d}t^2} = F$, for constant $F$.\n",
    "\n",
    "This equation does, of course, have a 2D space of solutions. This notebook finds the solution that minimizes the $L^2$ norm between the solution and a set of datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f558b4-aa0f-4a8f-aa36-e91cc9f50395",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b29c89-ca91-4ae2-9031-904fa7b4bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can edit anything in this cell.\n",
    "# If you do so, you should re-run the whole notebook.\n",
    "\n",
    "h = 0.01  # Grid resolution\n",
    "rng_seed_data = 1234  # Used for data generation\n",
    "sigma = 0.1  # Noise level for data generation\n",
    "N_samples = 10  # Number of datapoints to 'measure' from the 'true' curve\n",
    "F = -10  # Force. Constant in this version. Chosen to be close to free-fall in SI units.\n",
    "\n",
    "x0 = 0.5  # Initial condition for the true solution.\n",
    "v0 = 0.1  # Initial condition for the true solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fe83a2-4b30-43a8-9d3c-c19196e52269",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195e0e9-6302-4e12-84f4-9b60ee7ac9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_grid = np.arange(0, 1, h)  # Useful to have this for plotting.\n",
    "N_grid = t_grid.shape[0]  # Number of points in this discretized grid over t.\n",
    "x_true_grid = x0 + t_grid*v0 + 0.5*F*t_grid**2  # True solution curve.\n",
    "\n",
    "rng_data = np.random.RandomState(rng_seed_data)  # Instantiate the RNG in the same cell that we will do all the calls.\n",
    "idx_samples = rng_data.choice(N_grid, size=N_samples, replace=False)  # Choose which datapoints we will 'measure'\n",
    "idx_samples = np.sort(idx_samples)\n",
    "x_noise = rng_data.normal(scale=sigma, size=(N_samples,))  # Sample noise to be added to our datapoints.\n",
    "t_samples = t_grid[idx_samples]  # Used only for plotting in this example\n",
    "x_samples = x_true_grid[idx_samples] + x_noise  # Noisy datapoints\n",
    "del rng_data  # Delete to prevent re-use of this RNG in the solution section.\n",
    "\n",
    "# Plot the generated data\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t_grid, x_true_grid, ls='-', marker='none', label='True solution')\n",
    "ax.plot(t_samples, x_samples, ls='none', marker='o', alpha=0.7, label='Samples / measurements')\n",
    "ax.set_xlim(left=t_grid[0], right=t_grid[-1])\n",
    "ax.set_xlabel('t')\n",
    "ax.set_ylabel('x(t)')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c696d456-b9f6-4d24-9f81-8212d6462f26",
   "metadata": {},
   "source": [
    "# Numerical solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d17222-4e61-4dc7-8247-24a8c27f6e3b",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf4aed3-2dcc-4bcf-94e4-06b6740a638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters / initialization\n",
    "\n",
    "# Parameters in this cell are specific to the method of numerical solution.\n",
    "# If you change any of these, you do not need to re-run data generation.\n",
    "\n",
    "# The lines below define a linear transformation from the natural space of the problem,\n",
    "# i.e. the vector [x(t_0), x(t_1), ..., x(t_{N-1})],\n",
    "# to a potentially different space [z_0, z_1, ..., z_{N-1}] in which the ODE will be solved by gradient descent.\n",
    "# To solve this in the original space, set A_x2z to be the identity matrix.\n",
    "A_x2z = np.zeros((N_grid, N_grid), dtype=np.float64)\n",
    "A_x2z[0, 0] = 1.0  # z_0 = z(t_0)\n",
    "for i in range(1, N_grid):\n",
    "    # z_i = x(t_i) - x(t_{i-1}) for i > 0\n",
    "    A_x2z[i, i] = 1.0\n",
    "    A_x2z[i, i-1] = -1.0\n",
    "\n",
    "\n",
    "N_iter = 2000  # Number of iterations of optimization.\n",
    "\n",
    "# Initialize the solution grid.\n",
    "# I don't believe there is any benefit to using random initialization, since this problem does not\n",
    "# have the same requirement for symmetry-breaking that exists with the hidden neurons of a neural network.\n",
    "z_solution_grid = torch.zeros(N_grid, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "# Define the optimizer.\n",
    "# This problem seems to benefit from using a second-order optimizer (which LBFGS is), and I believe that\n",
    "# is due to the Hessian of loss_ODE (see below for defintion) having a very large condition number.\n",
    "optimizer = torch.optim.LBFGS(lr=1, history_size=10, params=[z_solution_grid])\n",
    "\n",
    "# Over the course of optimization we move from weighting the loss towards fitting the samples\n",
    "# to weighting the loss towards satisfying the ODE.\n",
    "# w_ODE = 0.0  <=> loss = L2 error of the samples\n",
    "# w_ODE = 1.0  <=> loss = L2 violation of the (discretized) ODE\n",
    "def get_w_ODE(iteration, N_iter):\n",
    "    if iteration < 0.1*N_iter:\n",
    "        # First 10% of steps: optimize mainly for fitting the samples\n",
    "        w_ODE = 0.01\n",
    "    elif iteration >= 0.9*N_iter:\n",
    "        # Final 90% of steps: optimize mainly for satisfying the ODE\n",
    "        w_ODE = 0.8\n",
    "    else:\n",
    "        # Linear ramp-up of w_ODE in between these iterations\n",
    "        w_ODE = 0.01 + 0.79*(iteration - 0.1*N_iter)/(0.8*N_iter)\n",
    "    return w_ODE\n",
    "\n",
    "# Logging frequency. Note that you may want to log scalars more frequently than the whole solution grid.\n",
    "logging_freq_scalars = 1\n",
    "logging_freq_grids = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7309900-77d0-4f5c-89ea-ac9435ec9673",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6d253-b811-43d8-8980-3320049c11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-changeable initialization. You should not change anything in this cell.\n",
    "\n",
    "# Invert the transformation matrix\n",
    "assert np.linalg.det(A_x2z) > 1e-4  # Throw on small determinant to guard against numerical non-invertibility.\n",
    "A_z2x = np.linalg.inv(A_x2z)\n",
    "A_z2x_torch = torch.tensor(A_z2x)  # We'll need this is in the loss\n",
    "\n",
    "# Put the 'samples' / 'measurements' in torch\n",
    "x_samples_torch = torch.tensor(x_samples)\n",
    "# ...and the indices they were sampled at.\n",
    "idx_samples_torch = torch.tensor(idx_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739de1a3-e339-4d5c-8d47-0a4a968f6485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function defintions\n",
    "\n",
    "# The loss for the optimization problem has two parts:\n",
    "# loss_data measures the l2 error of the data relative to our current solution x(t_0), x(t_1), ..., x(t_{N-1})\n",
    "# loss_ODE measures the l2 norm of the local violation of the ODE.\n",
    "# Our aim is to bring loss_ODE to zero while keeping loss_data as small as possible.\n",
    "\n",
    "# You should not change anything in this cell.\n",
    "\n",
    "# Note that loss_data is normalized by the number of samples.\n",
    "def loss_data(z):\n",
    "    x = A_z2x_torch @ z\n",
    "    x_at_sample_points = x[idx_samples_torch]\n",
    "    error_of_samples = (x_at_sample_points - x_samples_torch)\n",
    "    loss_val = 0.5*torch.sum(error_of_samples**2)\n",
    "    loss_val = loss_val / N_samples\n",
    "    return loss_val\n",
    "\n",
    "# Note that this loss is a sum over only the interior points, 1, 2, ..., N-2.\n",
    "# This is because we do not have sufficient data to compute the second derivative at points 0 and N-1.\n",
    "# This should be consistent with your intuition: if we simply demands that loss_ODE = 0, we would have N-2 equations in N unknowns.\n",
    "# This would (typically) have a two-dimensional space of solutions, which is what we should expect for a 2nd order ODE.\n",
    "def loss_ODE(z):\n",
    "    x = A_z2x_torch @ z\n",
    "    # Note the factor of h^-2. As h is increased (i.e. the grid is made finer) this should converge to the value of the\n",
    "    # second derivative (so long as x(t) is twice-differentiable).\n",
    "    second_deriv = h**(-2) * (x[:-2] - 2.0*x[1:-1] + x[2:])\n",
    "    rhs = F  # Scalar in the very simple case. Will get broadcast.\n",
    "    # Note the factor of h. This cancels out the implicit factor of N_grid from the sum.\n",
    "    # Alternatively, think of this loss as the (approximation to) the integral of the l2-violation of the ODE.\n",
    "    loss_val = 0.5*h*torch.sum((second_deriv - rhs)**2)\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dedbbd-c89c-4716-b4a8-a9ca4387c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "\n",
    "# Initialize logging history\n",
    "log_scalars = []\n",
    "log_grids = []\n",
    "\n",
    "for iteration in tqdm(range(N_iter)):\n",
    "    w_ODE = get_w_ODE(iteration, N_iter)\n",
    "\n",
    "    # Pass forward through the network\n",
    "    loss_data_torch = loss_data(z_solution_grid)\n",
    "    loss_ODE_torch = loss_ODE(z_solution_grid)\n",
    "    loss_total_torch = (1.0 - w_ODE) * loss_data_torch + w_ODE * loss_ODE_torch\n",
    "\n",
    "    # Store these for logging\n",
    "    loss_val_data = loss_data_torch.detach().item()\n",
    "    loss_val_ODE = loss_ODE_torch.detach().item()\n",
    "    loss_val_total = loss_total_torch.detach().item()\n",
    "\n",
    "    # Step the optimizer, updating z_solution_grid.\n",
    "    optimizer.zero_grad()\n",
    "    loss_total_torch.backward()\n",
    "    # Stepping the LBFGS optimizer requires a closure for evaluating the loss function\n",
    "    if type(optimizer) is torch.optim.LBFGS:\n",
    "        optimizer.step(lambda: (1.0 - w_ODE) * loss_data(z_solution_grid) + w_ODE * loss_ODE(z_solution_grid))\n",
    "    else:\n",
    "        optimizer.step()\n",
    "\n",
    "    # In many of my early experiments, the solution became NaN due to numerical instability.\n",
    "    # If this happens, it is useful to fail at this point. It is also helpful to know which iteration this happened at.\n",
    "    if z_solution_grid.isnan().any().item():\n",
    "        error_msg = f'NaNs appeared in solution after iteration {iteration}'\n",
    "        raise ValueError(error_msg)\n",
    "    \n",
    "    if iteration % logging_freq_scalars == 0:\n",
    "        log_scalars.append({\n",
    "            'iteration': iteration,\n",
    "            'w_ODE': w_ODE,\n",
    "            'loss_data': loss_val_data,\n",
    "            'loss_ODE': loss_val_ODE,\n",
    "            'loss_total': loss_val_total,\n",
    "        })\n",
    "\n",
    "    if iteration % logging_freq_grids == 0:\n",
    "        log_grids.append({\n",
    "            'iteration': iteration,\n",
    "            'z_grid': z_solution_grid.detach().numpy(),\n",
    "            'x_grid': (A_z2x_torch @ z_solution_grid).detach().numpy(),\n",
    "        })\n",
    "\n",
    "# Convert the solution back to x-space.\n",
    "# This may already be stored in the logs (if logging_freq_grids divides N_iter).\n",
    "x_solution_grid = (A_z2x_torch @ z_solution_grid).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f6c927-1909-4e19-8209-0bcb88cedb12",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef507d34-cba4-4ffd-a621-17b0e58f93fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the progress of the optimizer.\n",
    "# This is a useful diagnostic for figuring out if the solution has converged.\n",
    "# Based on this, you may want to go back and re-run the optimization with different\n",
    "# parameters (N_iter, get_w_ODE, a different optimizer, etc.)\n",
    "# This should eventually be replaced by some heuristics so that SR can be done without manual intervention.\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot([d['iteration'] for d in log_scalars], [d['loss_data'] for d in log_scalars],\n",
    "        marker='none', ls='-', color='tab:red', label='loss_data')\n",
    "ax.plot([d['iteration'] for d in log_scalars], [d['loss_ODE'] for d in log_scalars],\n",
    "        marker='none', ls='-', color='tab:blue', label='loss_ODE')\n",
    "ax.legend(loc='upper center')\n",
    "\n",
    "#ax.set_ylim(bottom=0.0, top=1)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iteration')\n",
    "ax.set_ylabel('loss')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot([d['iteration'] for d in log_scalars], [d['w_ODE'] for d in log_scalars],\n",
    "        marker='none', ls='--', color='tab:orange', label='w_ODE')\n",
    "ax2.set_ylabel('w_ODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b0350e-0578-4a7d-9a46-aa9128d26618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results of the optimization as x(t).\n",
    "# This is useful as a qualitative visual check that the optimization behaved as expected.\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t_grid, x_true_grid, ls='-', marker='none', label='True solution')\n",
    "ax.plot(t_samples, x_samples, ls='none', marker='o', alpha=0.7, label='Samples / measurements')\n",
    "ax.plot(t_grid, x_solution_grid, ls='--', marker='none', label='Optimization result solution')\n",
    "ax.set_xlim(left=t_grid[0], right=t_grid[-1])\n",
    "ax.set_xlabel('t')\n",
    "ax.set_ylabel('x(t)')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385c519-57e8-42e0-aa8b-c5fe066b9f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the second derivate against the RHS.\n",
    "# This is useful as a qualitative visual check that the optimization successfully imposed the ODE.\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "second_deriv_solution_grid = h**(-2) * (x_solution_grid[:-2] - 2.0*x_solution_grid[1:-1] + x_solution_grid[2:])\n",
    "ax.plot(t_grid[1:-1], second_deriv_solution_grid,\n",
    "        ls='-', marker='none', color='tab:green', label='Optization result (numerical) second derivative')\n",
    "ax.plot(t_grid[1:-1], [F for _ in t_grid[1:-1]],\n",
    "        ls='--', marker='none', color='tab:red', label='Expected RHS')\n",
    "ax.set_xlim(left=t_grid[0], right=t_grid[-1])\n",
    "ax.set_xlabel('t')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
