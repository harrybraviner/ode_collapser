{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86230ae-bdc8-440d-8b00-4e2f05dd0ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import scipy.integrate as integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1173433f-85cb-49db-8d06-b6c2a875b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _interpolate_between_samples(\n",
    "        t_grid: torch.Tensor,\n",
    "        idx_samples: torch.Tensor,\n",
    "        x_samples: torch.Tensor,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a linear interpolation between the points x_samples at the grid points t_grid[idx_samples].\n",
    "\n",
    "    :param t_grid: Grid of times.\n",
    "    :param idx_samples: Indices of times that x values are samples from.\n",
    "    :param x_samples: Samples at times. Shape must match idx_samples.\n",
    "    :return: Interpolated x values, should match the shape of t_grid.\n",
    "    \"\"\"\n",
    "    x_interpolated = torch.tile(input=x_samples[0, :], dims=(t_grid.shape[0], 1))\n",
    "    for j, (i1, i2) in enumerate(zip(idx_samples[:-1], idx_samples[1:])):\n",
    "        aa = x_samples[j, :] + (x_samples[j + 1, :] - x_samples[j, :]) * ((\n",
    "                    t_grid[i1 + 1:i2 + 1] - t_grid[i1]) / (t_grid[i2] - t_grid[i1])).reshape((-1, 1))\n",
    "        x_interpolated[i1 + 1: i2 + 1, :] = aa\n",
    "    x_interpolated[idx_samples[-1] + 1:, :] = x_samples[-1, :]\n",
    "    return x_interpolated\n",
    "\n",
    "\n",
    "def collapse_to_solution(\n",
    "    rhs,\n",
    "    h,\n",
    "    t_start,\n",
    "    t_end,\n",
    "    idx_samples,\n",
    "    x_samples,\n",
    "    dim,\n",
    "    transformation_x2z=None,\n",
    "    N_iter=5000,\n",
    "    get_w_ODE=None,\n",
    "    initialize_by_interpolation=True,\n",
    "    logging_freq_scalars=1,\n",
    "    logging_freq_grids=10,\n",
    "    show_progress=False,\n",
    "    get_optimizer_from_params=None,\n",
    "):\n",
    "    # Get the number of grid points\n",
    "    t_grid = torch.arange(t_start, t_end, h)\n",
    "    n_grid = t_grid.shape[0]\n",
    "\n",
    "    # If the inputs are not torch tensors, try and make tensors from them.\n",
    "    assert x_samples.shape[1] == dim\n",
    "    assert idx_samples.shape[0] == x_samples.shape[0]  # FIXME - will need to modify in the case of vector x with partial observations\n",
    "    if not isinstance(idx_samples, torch.Tensor):\n",
    "        idx_samples = torch.tensor(idx_samples, requires_grad=False)\n",
    "    if not isinstance(x_samples, torch.Tensor):\n",
    "        x_samples = torch.tensor(x_samples, requires_grad=False)\n",
    "    # FIXME - check that idx_samples are sorted. Else do dual-sort of idx_samples and x_samples\n",
    "\n",
    "\n",
    "    # I believe this is necessary to avoid errors if we ever move the tensors to the gpu\n",
    "    # (and to avoid a deprecation warning even if they are on the cpu).\n",
    "    rhs = torch.vmap(rhs, in_dims=(0, 0), out_dims=0)\n",
    "    #rhs = torch.compile(rhs)\n",
    "\n",
    "    if transformation_x2z is None:\n",
    "        # # The default option transforms the problem into gradient-space plus a constant.\n",
    "        # transformation_x2z = torch.zeros((n_grid, n_grid), dtype=torch.float64)\n",
    "        # transformation_x2z[0, 0] = 1.0  # z_0 = x(t_0)\n",
    "        # for i in range(1, n_grid):\n",
    "        #     # z_i = x(t_i) - x(t_{i-1}) for i > 0\n",
    "        #     transformation_x2z[i, i] = 1.0\n",
    "        #     transformation_x2z[i, i - 1] = -1.0\n",
    "        \n",
    "        # Default transformation is the identity\n",
    "        transformation_x2z = torch.eye(n_grid, dtype=torch.float64)\n",
    "    else:\n",
    "        if not isinstance(transformation_x2z, torch.Tensor):\n",
    "            transformation_x2z = torch.tensor(transformation_x2z)\n",
    "\n",
    "    # Default loss-weighting schedule\n",
    "    if get_w_ODE is None:\n",
    "        def get_w_ODE(it, n_iterations):\n",
    "            if it < 0.1 * n_iterations:\n",
    "                # First 10% of steps: optimize mainly for fitting the samples\n",
    "                w_ode = 1e-2\n",
    "            elif it >= 0.9 * n_iterations:\n",
    "                # Final 90% of steps: optimize mainly for satisfying the ODE\n",
    "                w_ode = 1.0\n",
    "            else:\n",
    "                # Linear ramp-up of w_ODE in between these iterations\n",
    "                w_ode = 1e-2 + (1 - 1e-2) * (it - 0.1 * n_iterations) / (0.8 * n_iterations)\n",
    "            return w_ode\n",
    "\n",
    "    # Invert the transformation matrix. We will need this repeatedly later.\n",
    "    if torch.linalg.det(transformation_x2z).item() < 1e-4:\n",
    "        error_msg = f'Transformation matrix from [x(t_i)] to [z_i] has a small determinant.'\n",
    "        raise ValueError(error_msg)\n",
    "    transformation_z2x = torch.linalg.inv(transformation_x2z)\n",
    "\n",
    "    # Initialize the solution grid.\n",
    "    # FIXME - do I need to change this in the case of partial observations?\n",
    "    #         e.g. what if I get (x, xdot) at point i, but only x at point i+1?\n",
    "    # I don't believe there is any benefit to using random initialization, since this problem does not\n",
    "    # have the same requirement for symmetry-breaking that exists with the hidden neurons of a neural network.\n",
    "    if initialize_by_interpolation:\n",
    "        # FIXME - need to make _interpolate_between_samples work in multiple dimensions even for complete data.\n",
    "        x_interpolated = _interpolate_between_samples(t_grid, idx_samples, x_samples)\n",
    "        z_solution_grid = (transformation_x2z @ x_interpolated).detach().clone().to(torch.float64).requires_grad_(True)\n",
    "    else:\n",
    "        z_solution_grid = torch.zeros((n_grid, dim), dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    # Initialize the optimizer.\n",
    "    # This problem seems to benefit from using a second-order optimizer (which LBFGS is), and I believe that\n",
    "    # is due to the Hessian of loss_ODE (see below for definition) having a very large condition number.\n",
    "    if get_optimizer_from_params is None:\n",
    "        optimizer = torch.optim.LBFGS(lr=1, history_size=10, params=[z_solution_grid])\n",
    "    else:\n",
    "        optimizer = get_optimizer_from_params([z_solution_grid])\n",
    "\n",
    "    # Loss function definitions\n",
    "\n",
    "    # The loss for the optimization problem has two parts:\n",
    "    # loss_data measures the l2 error of the data relative to our current solution x(t_0), x(t_1), ..., x(t_{N-1})\n",
    "    # loss_ODE measures the l2 norm of the local violation of the ODE.\n",
    "    # Our aim is to bring loss_ODE to zero while keeping loss_data as small as possible.\n",
    "\n",
    "    # Note that loss_data is normalized by the number of samples.\n",
    "    # FIXME - add an option to weight data points (and dimensions within points)\n",
    "    # FIXME - does this work in multiple dimensions (genuinely unsure)\n",
    "    def loss_data(z):\n",
    "        x = transformation_z2x @ z\n",
    "        x_at_sample_points = x[idx_samples, :]\n",
    "        error_of_samples = (x_at_sample_points - x_samples)\n",
    "        loss_val = 0.5 * torch.mean(error_of_samples ** 2)\n",
    "        return loss_val\n",
    "\n",
    "    # Note that this loss is a sum over only the interior points, 0, 2, ..., N-2.\n",
    "    # This is because we do not have sufficient data to compute the forward derivative at N-1.\n",
    "    # This should be consistent with your intuition: if we simply demand that loss_ODE = 0,\n",
    "    # we would have d*(N-1) equations in d*N unknowns.\n",
    "    # This would (typically) have a d-dimensional space of solutions,\n",
    "    # which is what we should expect for a 1st order, d-dimensional ODE.\n",
    "    def loss_ODE(z):\n",
    "        x = transformation_z2x @ z\n",
    "        first_deriv = h**(-1) * (x[1:] - x[:-1])\n",
    "        rhs_val = rhs(0.5*(x[1:] + x[:-1]), 0.5*(t_grid[1:] + t_grid[:-1]))\n",
    "        # Note the factor of h. This cancels out the implicit factor of n_grid from the sum.\n",
    "        # Alternatively, think of this loss as the (approximation to) the integral of the l2-violation of the ODE.\n",
    "        loss_val = 0.5 * h * torch.sum((first_deriv - rhs_val) ** 2)\n",
    "        return loss_val\n",
    "\n",
    "    # Optimization takes place below here\n",
    "\n",
    "    # Initialize logging history\n",
    "    log_scalars = []\n",
    "    log_grids = []\n",
    "\n",
    "    success_flag = True  # Set to false if an error condition is encountered\n",
    "    failure_reason = None  # Should be set if success_flag gets set to False\n",
    "    iterations = tqdm(range(N_iter)) if show_progress else range(N_iter)\n",
    "    for iteration in iterations:\n",
    "        w_ODE = get_w_ODE(iteration, N_iter)\n",
    "\n",
    "        # Pass forward through the network\n",
    "        loss_data_torch = loss_data(z_solution_grid)\n",
    "        loss_ODE_torch = loss_ODE(z_solution_grid)\n",
    "        #loss_total_torch = (1.0 - w_ODE) * loss_data_torch + w_ODE * loss_ODE_torch\n",
    "        loss_total_torch = w_ODE*loss_ODE_torch\n",
    "\n",
    "        # Store these for logging\n",
    "        loss_val_data = loss_data_torch.detach().item()\n",
    "        loss_val_ODE = loss_ODE_torch.detach().item()\n",
    "        loss_val_total = loss_total_torch.detach().item()\n",
    "\n",
    "        # Step the optimizer, updating z_solution_grid.\n",
    "        optimizer.zero_grad()\n",
    "        loss_total_torch.backward()\n",
    "        # Stepping the LBFGS optimizer requires a closure for evaluating the loss function\n",
    "        if type(optimizer) is torch.optim.LBFGS:\n",
    "            #optimizer.step(lambda: (1.0 - w_ODE) * loss_data(z_solution_grid) + w_ODE * loss_ODE(z_solution_grid))\n",
    "            optimizer.step(lambda: w_ODE * loss_ODE(z_solution_grid))\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        # In many of my early experiments, the solution became NaN due to numerical instability.\n",
    "        # If this happens, it is useful to fail at this point.\n",
    "        # It is also helpful to know which iteration this happened at.\n",
    "        if z_solution_grid.isnan().any().item():\n",
    "            error_msg = f'NaNs appeared in solution after the gradient descent step of iteration {iteration}'\n",
    "            failure_reason = error_msg\n",
    "            success_flag = False\n",
    "        \n",
    "        # After taking the optimization step, exactly solve for the minimum of L_data in the plane\n",
    "        # parallel to the gradient.\n",
    "        a = z_solution_grid.grad  # FIXME - this should really take place in x-space. Getting away with this because I'm\n",
    "                                  #         using trans_x2z = eye right now.\n",
    "        a = a[idx_samples]  # We only solve the quadratic L_data minimization problem in the subspace of sampled points.\n",
    "                            # The other directions have zero gradient in L_data, and make the solution non-unique.\n",
    "                            # FIXME - can I come up with a constraint that renders the solution still unique?\n",
    "        a = a.flatten()\n",
    "        # Optimize over the x . grad = b plane. Therefore we must compute b.\n",
    "        b = torch.dot(x_samples.flatten(), a)\n",
    "        # We don't need to numerically invert a matrix for this problem,\n",
    "        # since it's possible to exactly compute the inverse matrix.\n",
    "        inv_matrix = torch.eye(len(idx_samples)+1, dtype=torch.float64)\n",
    "        inv_a2 = 1/(torch.linalg.norm(a, ord=2))**2\n",
    "        inv_matrix[:-1, :-1] -= inv_a2*torch.outer(a, a)\n",
    "        inv_matrix[:-1, -1] = inv_a2*a\n",
    "        inv_matrix[-1, :-1] = inv_a2*a\n",
    "        inv_matrix[-1, -1] = -inv_a2\n",
    "        rhs_vec = torch.cat([x_samples.flatten(), torch.tensor([b])])\n",
    "        minimizer = inv_matrix @ rhs_vec\n",
    "        # FIXME - only works right now because we are using trans_x2z = eye.\n",
    "        with torch.no_grad():\n",
    "            z_solution_grid[idx_samples] = minimizer[:-1].reshape((-1, 1))\n",
    "\n",
    "        # In many of my early experiments, the solution became NaN due to numerical instability.\n",
    "        # If this happens, it is useful to fail at this point.\n",
    "        # It is also helpful to know which iteration this happened at.\n",
    "        if z_solution_grid.isnan().any().item():\n",
    "            det_inv_matrix = torch.linalg.det(inv_matrix)\n",
    "            error_msg = f'NaNs appeared in solution after the quadratic optimization step of iteration {iteration}.'\n",
    "            # error_msg += f'\\nMatrix had determinant {det_inv_matrix}:\\n'\n",
    "            # error_msg += f'{inv_matrix}'\n",
    "            # error_msg += '\\nRestricted gradient vector:\\n'\n",
    "            # error_msg += f'{a}'\n",
    "            failure_reason = error_msg\n",
    "            success_flag = False\n",
    "\n",
    "        if iteration % logging_freq_scalars == 0:\n",
    "            log_scalars.append({\n",
    "                'iteration': iteration,\n",
    "                'w_ODE': w_ODE,\n",
    "                'loss_data': loss_val_data,\n",
    "                'loss_ODE': loss_val_ODE,\n",
    "                'loss_total': loss_val_total,\n",
    "            })\n",
    "\n",
    "        if iteration % logging_freq_grids == 0:\n",
    "            log_grids.append({\n",
    "                'iteration': iteration,\n",
    "                'z_grid': z_solution_grid.detach().numpy(),\n",
    "                'x_grid': (transformation_z2x @ z_solution_grid).detach().numpy(),\n",
    "            })\n",
    "\n",
    "        # Break out of the loop if we have encountered a failure reason\n",
    "        if success_flag == False:\n",
    "            break\n",
    "\n",
    "    # Convert the solution back to x-space.\n",
    "    # This may already be stored in the logs (if logging_freq_grids divides N_iter).\n",
    "    x_solution_grid = (transformation_z2x @ z_solution_grid).detach().numpy()\n",
    "\n",
    "    return {\n",
    "        'success': success_flag,\n",
    "        'failure_reason': failure_reason,\n",
    "        'x_solution_grid': x_solution_grid,\n",
    "        'log_scalars': log_scalars,\n",
    "        'log_grids': log_grids,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714ed74-3686-4b94-936c-5c3f67c8bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_ODE(it, N_iterations):\n",
    "    return 1.0\n",
    "\n",
    "def get_optim(params):\n",
    "    return torch.optim.SGD(lr=1e-3, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e20f3-71c0-4fe7-bd02-281ffe292235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_rhs(x, t):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa7e94-71fe-496d-9f37-9c63c28475e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.1\n",
    "n_samples = 100\n",
    "\n",
    "t_start = 0.0\n",
    "t_end = 1.0\n",
    "h = 0.01\n",
    "t_grid = np.arange(t_start, t_end, h)\n",
    "x_grid = np.exp(t_grid).reshape((-1, 1))\n",
    "n_grid = t_grid.shape[0]\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "idx_samples = rng.choice(n_grid, size=n_samples, replace=False)\n",
    "idx_samples = np.sort(idx_samples)\n",
    "x_samples = x_grid[idx_samples, :] + rng.normal(size=(n_samples, 1), scale=sigma)\n",
    "t_samples = t_grid[idx_samples]\n",
    "del rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e00d6-f2dc-4941-811a-c8cdd588bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapser_results = collapse_to_solution(\n",
    "    f_rhs,\n",
    "    h=h,\n",
    "    t_start=0.0,\n",
    "    t_end=1.0,\n",
    "    idx_samples=idx_samples,\n",
    "    x_samples=x_samples,\n",
    "    dim=1,\n",
    "    get_w_ODE=get_w_ODE,\n",
    "    get_optimizer_from_params=get_optim,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9be696-1499-4749-9ac9-619c8be4a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [collapser_results['log_scalars'][i]['loss_data'] for i in range(len(collapser_results['log_scalars']))],\n",
    "    ls='-', marker='none', color='tab:blue'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7295d12f-7464-4a8e-b16d-13ed895cd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [collapser_results['log_scalars'][i]['loss_ODE'] for i in range(len(collapser_results['log_scalars']))],\n",
    "    ls='-', marker='none', color='tab:orange'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3692b45b-b41c-406e-baa4-c5b2a5d608e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(t_grid, x_grid, ls='-', marker='none', color='tab:blue')\n",
    "ax.plot(t_samples, x_samples, ls='none', marker='o', color='tab:orange')\n",
    "ax.plot(t_grid, collapser_results['x_solution_grid'], ls='--', marker='none', color='tab:green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f485c9ad-e717-4a1e-bb45-b0cc782d9061",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot([collapser_results['log_scalars'][i]['loss_data'] for i in range(len(collapser_results['log_scalars']))])\n",
    "ax.plot([collapser_results['log_scalars'][i]['loss_ODE'] for i in range(len(collapser_results['log_scalars']))])\n",
    "#ax.set_ylim(0.0, 1.0)\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93aea4b-9796-4790-8ccb-a2cf64267f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for xdot = -(x-1)*x*(x+1)\n",
    "\n",
    "def f_rhs(x, t):\n",
    "    return -(x - 1)*x*(x + 1)\n",
    "\n",
    "\n",
    "def soln(t):\n",
    "    return 1/np.sqrt(1 - np.exp(-2*(t+0.1)))\n",
    "\n",
    "sigma = 0.1\n",
    "n_samples = 10\n",
    "\n",
    "t_start = 0.0\n",
    "t_end = 1.0\n",
    "h = 0.01\n",
    "t_grid = np.arange(t_start, t_end, h)\n",
    "x_grid = soln(t_grid).reshape((-1, 1))\n",
    "n_grid = t_grid.shape[0]\n",
    "\n",
    "\n",
    "# Get numerical solution\n",
    "ode_integrator = integrate.ode(f = lambda t, y:f_rhs(y, t))\n",
    "ode_integrator.set_initial_value(np.array([1 / np.sqrt(1 - np.exp(-0.2))]))\n",
    "x_numeric = np.zeros_like(t_grid)\n",
    "x_numeric[0] = ode_integrator.y[0]\n",
    "for i, t in enumerate(t_grid[1:]):\n",
    "    x_numeric[i+1] = ode_integrator.integrate(t)[0]\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "idx_samples = rng.choice(n_grid, size=n_samples, replace=False)\n",
    "idx_samples = np.sort(idx_samples)\n",
    "x_samples = x_grid[idx_samples, :] + rng.normal(size=(n_samples, 1), scale=sigma)\n",
    "t_samples = t_grid[idx_samples]\n",
    "del rng\n",
    "\n",
    "collapser_results = collapse_to_solution(\n",
    "    f_rhs,\n",
    "    h=h,\n",
    "    t_start=0.0,\n",
    "    t_end=1.0,\n",
    "    idx_samples=idx_samples,\n",
    "    x_samples=x_samples,\n",
    "    dim=1,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(t_grid, x_grid, ls='-', marker='none', color='tab:blue')\n",
    "ax.plot(t_samples, x_samples, ls='none', marker='o', color='tab:orange')\n",
    "ax.plot(t_grid, collapser_results['x_solution_grid'], ls='--', marker='none', color='tab:green')\n",
    "ax.plot(t_grid, x_numeric, ls='--', marker='none', color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f186a98c-f311-41ec-8b4f-5fb0ec352c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
