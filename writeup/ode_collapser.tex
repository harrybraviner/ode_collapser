\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{url}
\usepackage{graphicx}
\title{Fitting ODE solutions to noisy data by collapsing onto the manifold of solutions}
\author{Harry Braviner}

\date{\today}

\begin{document}
\maketitle

\section{Statement of the problem}

We have a collection of datapoints, $\left\{ \tilde{x}(t_i) \right\}$, and an ODE, $\mathcal{D}\left[ x; t \right] = 0$. We believe that the datapoints \emph{may be} noisy samples from \emph{some} solution to this ODE, but we do \emph{not} have a guess for the intial or bounday conditions. How can we evaluate the goodness-of-fit of this ODE (which we probably cannot solve analytically, and which we do not have initial conditions for) to these datapoints?

Let's simplify the problem space by making two big restrictions:
\begin{enumerate}
\item We only consider only one-dimensional second-order ODEs. i.e. the differential operator $\mathcal{D}\left[x ; t \right]$ has the form $\ddot{x}(t) - f\left(x(t), \dot{x}(t), t\right)$.
\item The datapoints, $\tilde{x}(t)$, are samples from an evenly-spaced discrete grid of times, $t_0, t_1, \dots, t_{N-1}$, where $t_i - t_{i-1} = h$. The samples themselves need not be evenly spaced: e.g. our dataset could consist of $\left\{\tilde{x}(t_0), \tilde{x}(t_1), \tilde{x}(t_2), \tilde{x}(t_{50})\right\}$. We also assume that we have at most one sample from any $t_i$. i.e. we never have multiple samples from the same time with different noise components.
\end{enumerate}

I am not concerned at this point about where $f$ comes from.
The motivation for everything in this document is that I would like to be able to modify PySR to search for such an $f$.
Doing so requires%
\footnote{It also requires modifying PySR to output functions of $t$, $x(t)$, and $\dot{x}(t)$, but I believe that is comparatively straightforward: have PySR treat $t$, $x(t)$ and $\dot{x}(t)$ as three independent variables.}
evaluating the goodness-of-fit of each $f$ to the data, and this work is intended to provide a method of doing so.

The essential idea I shall outline is this: We will approximate the (infinite dimensional) space of functions on $\left[t_{\mathrm{start}}, t_{\mathrm{end}} \right]$ by an $N$-dimensional discretization
$$
\left( \hat{x}(t_0), \hat{x}(t_1), \dots, \hat{x}(t_{N-1} \right)
$$
Within this $N$-dimensional space there is some subspace (the \emph{solution space}) that satisfies the ODE.%
\footnote{
And for a one-dimensional second-order ODE this subspace should generally be 2-dimensional.
}
\textbf{We shall collapse the $N$-dimensional discretization onto the solution space \emph{along the path minimizing the MSE of the data}.}
I shall be more precise about what this means in section \ref{sec:method}, after briefly reviewing other work and ideas.

\subsection{Previous work}

I haven't done a very comprehensive literature review, so I can't rule out that someone has done something similar to myself.
However, a little googling hasn't turned up anything identical.
I'll review here what I \emph{did} find, since a lot of this is quite interesting, might provide inspiration along different directions, and I want to at least have it written down for my own purposes.

In \cite{kronberger2019identification} the authors consider symbolic regression of first-order autonomous dynamical systems:
\begin{align}
\dot{x}_1(t) &= f_1\left(x_1, \dots, x_n\right) \nonumber \\
\dot{x}_2(t) &= f_2\left(x_1, \dots, x_n\right) \nonumber \\
 & \vdots \nonumber \\
\dot{x}_n(t) &= f_n\left(x_1, \dots, x_n\right) \nonumber \\
\end{align}
The candidate system, $\left( f_1, f_2, \dots, f_n\right)$, is the output of a genetic algorithm, although it does not appear to be PySR.
The authors use two methods of evaluating the goodness-of-fit of the ODE to the datapoints:
\begin{enumerate}
\item Using the approximate derivative values as a target, i.e.
$$
\mathrm{Loss}(f) = \sum_{i, j} \bigg(f_i\left( x_1(t_j), \dots, x_n(t_j) \right) - \big(x_i(t_{j+1}) - x_i(t_j)\big) \bigg)^2
$$
\item Integrating from the initial values $(x_1(t_0), ..., x_n(t_0))$ using a numerical ODE solver.
\end{enumerate}
The method of \cite{kronberger2019identification} does not appear to allow for noise in the measurements.
The results section shows the method being less successful on motion-captured data vs simulated data, and this supports my belief that this method does not tolerate measurement noise well.
They also assume that their datapoints are evenly spaced in time, although I'm not sure this is necessary. However, for method (1) to work, it is necessary for the spacing between the datapoints to be such that the numerical gradient is a good approximation to the true gradient.

\cite{bongard2007automated} also appears to integrate the candidate ODE from initial conditions that are assumed to be know.
The authors describe a system which not only evaluates candidate dynamical systems, but also suggests new experiments (i.e. initial conditions) that should best disambiguate between currently-strong candidates.
While this is a useful capability, and possibly the reason that the authors assume the initial conditions are known (because you set up the exeriment), it neglects the fact that in reality we never quite set up the system as we intend to.
The paper also assumes that the system is first order.

In \cite{iba2008inference} it is not clear how the authors obtain initial conditions, but I think that they simply take the same initial conditions used to generate the data and pass these, along with the candidate system, to the ODE integrator. Again, this paper condiders first order dynamical systems.

In \cite{d2023odeformer} the authors implement an end-to-end deep learning approach.
Input observations (i.e. $(x_1(t_i), \dots x_n(t_i), t_i)$) are embedded and then passed to a transformer, which outputs $(f_1, \dots, f_n)$ in the form of a string of symbols representing the tree form of the $f$s in prefix notation.
The model is trained on a randomly generated synthetic dataset.
This has the advantage that the target string is known (because it was generated at random), and the input data (the samples) are generated by numerical integration of the $f$.
The authors do consider noisy data and non-uniform spacing ('subsampling').
This paper completely sidesteps the issue of how to evaluate the goodness-of-fit without the underlying initial conditions: we do not have to repeatedly evaluate the goodness-of-fit of $f$s, because we only have a single candidate, the output of the transformer.
Unlike PySR, this method does not produce a Pareto front.

The issue \cite{pysrissue568} on the PySR github repo suggests that, at least as of March 2024, the accepted method for performing SR for an ODE was haveing PySR generate both the ODE and the initial conditions.
This has two issues.
First, the additional time taken to search for initial conditions (and all of the problems I will discuss in the next section regarding sweep-and-shoot approaches).
Secondly, outputting an ODE and an intial condition allows us to test only a single dataset.
We might have multiple timeseries that we believe obey the same ODE with different initial conditions.

In issue \cite{pysrissue732} Miles refers someone looking to do SR on ODEs to \cite{pysrchangenotev1.3.0}, which suggests performing integration of a function by generating data points for the integrand, and then performing SR on candidate expressions for the integral.
Could this method be applied to an ODE (by replacing the dataset generated from the integrand in \cite{pysrchangenotev1.3.0} by the timeseries)?
One limitation of doing so is that we are now searching over the expression for the \emph{solution to} the ODE.
Many physical systems have a simple expression in terms of ODEs, but no closed form solution to that ODE.

\subsection{Alternative methods}

In this section I'm going to outline some alternative methods that seem attractive.

\subsubsection{Extracting $\dot{x}(t)$ from the data}
\label{sec:alternate_extracting_xdot}

Several of the papers described above evaluate $f$ directly as the degree of disagreement between $f(x(t))$ and $\dot{x}(t)$ as approximated by forward differences from the data samples.
Even if the samples have zero noise, the descretization of the time-sampling can cause considerable error in the approximation (see figure \ref{fig:deriv_from_samples}), and the addition of noise makes the error much worse (see figure \ref{fig:deriv_from_noisy_samples}).
The method I outline in the note should be able to handle much sparser and noisier samples.

\begin{figure}
\includegraphics{images/alternative_methods/deriv_from_samples.png}
\centering
\caption{Forward numerical derivative, $\left(x(t_{i+1}) - x(t_i)\right)/\left(t_{i+1} - t_i\right)$ from sampled data. There is some error, with the derivative being shifted left, even though the points are densely spaced.}
\label{fig:deriv_from_samples}
\end{figure}

\begin{figure}
\includegraphics{images/alternative_methods/deriv_from_noisy_samples.png}
\centering
\caption{Forward numerical derivative, $\left(x(t_{i+1}) - x(t_i)\right)/\left(t_{i+1} - t_i\right)$ from sampled data with Gaussian noise with $\sigma = 0.05$. The error is considerably worse than in figure \ref{fig:deriv_from_samples}.}
\label{fig:deriv_from_noisy_samples}
\end{figure}

\subsection{Smoothing the data with a kernel}
\label{sec:smoothing}

Smoothing the datapoints with some kernel function (e.g. a rolling average window) would go some way toward addressing the issue of noise in the dataset.
PySR supports something similar to this feature via the \texttt{denoise} option, which first fits a Gaussian process to the data.
This could be modified to also output the gradient of the fitted process.
One downside of this method is that it requires the choice of hyperparameters for the kernel of the Gaussian process (I don't think PySR exposes these choices to the end user, and I'm not sure if it has some clever way to pick them).

A second, more subtle problem exists with both this method and that of section \ref{sec:alternate_extracting_xdot}. If the data sampling is sufficiently irregular, checking that the candidate ODE is obeyed locally is \emph{not} the same as verifying that it is obeyed globally.
This is illustrated in figure \ref{fig:gp_fitting}.
A Gaussian process is fitted to two clusters of sampled datapoints.
Comparing the second derivative of the Gaussian process to the right hand side of $\ddot{x}(t) = -(2\pi10)^2 x(t)$ gives excellent agreement, and we might conclude that this data comes from a 10Hz harmonic oscillator.
But it does not.
In the upper plot of figure \ref{fig:gp_fitting} we also show two solutions for a 10Hz harmonic oscillator, with different initial conditions.
It is clear that one cluster of sampled datapoints fits one set of initial conditions, and the other cluster fits the other set.
But the whole cluster as a single timeseries does not fit this candidate ODE for any set of initial condtions.

\begin{figure}
\includegraphics{images/alternative_methods/gp_fitting.png}
\centering
\caption{A Gaussian process is fitted through sampled datapoints (top plot), and the second derivative of this process is compared to the right-hand side of the candidate equation $\ddot{x}(t) = -(2\pi10)^2 x(t)$ (bottom plot) where excellent agreement is seen (the relative error is a few parts-per-thousand).
However, the two clusters of datapoints come from solutions of this ODE that have different initial conditions, being $180^{\circ}$ out of phase (dotted lines on the top plot).
}
\label{fig:gp_fitting}
\end{figure}

We should note that this is not an artifact of the Gaussian process.
This issue would arise even if we had access to measured values of $\ddot{x}(t)$ at the sample points.

One could object that I have cheated by only evaluating the Gaussian process and its second derivative at the sampled points.
Should we have compared $\hat{x}$ from the Gaussian process to its second derivative over the whole range?
I believe not.
Doing so here would show serious divergence from the ODE $\ddot{x} = -(2\pi10)^2 x$ in the middle of the domain.
\emph{But it would also do so if the datapoints were drawn from the same solution.}
We would mark \emph{any} candidate ODE as a poor fit to data with large gaps in its time-sampling.
The Gaussian process only helps us \emph{interpolate}; it is not reasonable to expect it to also \emph{extrapolate}.

\subsection{Sweep-and-shoot methods}
\label{sec:sweep_and_shoot}

If we knew the intial conditions for our dataset, we could integrate from the earliest point using a numerical ODE solver and compare the numerically integrated solution, $\hat{x}(t)$, to the sampled solution $\tilde{x}(t)$, at every later sampling time.
This works well if the initial conditions are known with zero noise.
In practice, noise may be present, and this method privileges the first point (by treating it as noiseless) in a way that we would probably prefer to avoid.
In the case of second order ODEs, we need to either have direct measurement of $\tilde{\dot{x}}$ or use the first two points to approximate it.

In the absence of noiseless initial conditions, we can `sweep' over plausible values and integrate the ODE forward in time (`shoot') for each set of initial conditions.
The noisy values for the first $x$ and $\dot{x}$ could be used as a point around which to centre the sweep.

This approach is viable for a low dimensional problem, but will become exponentially expensive as the number of dimensions increases.
If we choose to sweep over $10$ values for each inital condition for a 3 dimensional second order ODE ($x$, $\dot{x}$, $y$, $\dot{y}$, $z$, $\dot{z}$) we would have $10^6$ combinations to try.

Even in low-dimensional problem, sweep-and-shoot can struggle to evaluate whether the ODE is a good fit to the data. Consider the system
$$
\ddot{x}(t) = - x(t)^3 + a^2 x(t) - \nu \dot{x}(t)
$$
This system is a double-well potential with stable fixed points at $x = \pm a$ (with small oscillation frequencies of $\sqrt{2 a^2 - \nu^2/4}$) and an unstable fixed point at $x=0$ (with $x(t) \approx A_0 \exp(a (t - t_0)$ around this point).

\begin{figure}
\includegraphics{images/alternative_methods/double_well.png}
\centering
\caption{
Three (numerically integrated) solutions to $\ddot{x}(t) = -x(t)^2 + a^2 x(t) - \nu \dot{x}(t)$ for $a = 6\pi / \sqrt{2}$ and $\nu = 3$.
Note that the initial conditions have identical velocities and differ very small values of the $x$ coordinate.
}
\label{fig:double_well}
\end{figure}

Figure \ref{fig:double_well} shows three solutions to this system, for very similar initial conditions.
Suppose that our experiment had evolved along the dashed, orange curve ($x_0 = 10^{-11}$, $v_0 = 0$), and we had samples from it.
If we were performing a sweep-and-shoot search, we might well have integrated from $x_0 = 10^{-3}$, produced the blue curve, and measured this to have high MSE relative to our data.
We would then step our initial condition to our next point, $x_0 = -10^{-3}$, and produce the green curve, having an even higher MSE.
We would reject the candidate ODE, despite it being completely correct!

One might object to this, saying that clearly the sweep should be performed in log-space since we are so close to $x=0$.
This is true, but we know this because we are applying our intelligence and knowledge of the underlying ODE.
The location of an unstable fixed point will not always be at zero.
To first locate the fixed points of the candidate ODE and treat them as special in a sweep-and-shoot procedure would be considerable additional algorithmic complexity.
(In fact, taking the initiative to do so might be a good desideratum of an AI physicist!)

\subsection{Relaxation methods}

The method I describe below will look a bit like a relaxation method (see \cite{wikipediaRelaxationMethod}), and one might wonder if some variation of this could be used to achieve faster convergence.
This might be the case, but if so I haven't yet figured out how.

The relaxation method seems to require fixed, known end-points.
i.e. it requires boundary conditions.
We could take the earliest and last sampled datapoints, and use these as boundary conditions.
However, that would (just like shooting from the first point) elevate these points in significance by assuming that they have no measurement noise.
We would also have a version of the problem indicated in figure \ref{fig:double_well}: the blue and orange curves have extremely similar boundary conditions if $t_{\mathrm{start}}$ is sufficiently early, $t_{\mathrm{end}}$ significantly late.

\section{The Collapse Method}
\label{sec:method}

There are two different ways to think of the problem of how well an ODE fits a sampled dataset. Every paper I have been able to find on this subject implicitly does one or the other, but none of them seem to discuss the fact that the other viewpoint exists, or even explicitly acknowledge their choice.
\begin{enumerate}
\item Fit the data exactly and measure the extent to which the ODE is violated. e.g. $L_{\mathrm{ODE}} = \sum_i \left( \tilde{\ddot{x}}(t_i) - f(\tilde{x}(t_i), \tilde{\dot{x}}(t_i), t_i)\right)^2$.
This is the approch of most papers that approximate the derivative by finite differences between datapoints (though, as we discussed in section \ref{sec:smoothing}, you could fit a smoothing function instead).
\item Insist on the ODE being obeyed, and measure the error of the data relative to the solution, e.g. $L_{\mathrm{data}} = \sum_i \left( \hat{x}(t_i) - \tilde{x}(t_i) \right)^2$.
This is the approach of papers where the candidate ODE is numerically integrated. It requires some way of obtaining initial conditions. 
\end{enumerate}

\begin{figure}
\includegraphics{images/method/disc_space_viz.png}
\centering
\caption{
Schematic representation of discretized solution space.
Gray dashed curves are contours lines of the data loss, $L_{\mathrm{data}}$.
The solid blue dot shows the (global and only) minimum of $L_{\mathrm{data}}$.
The heavy green curve is the solution space, and the orange circles denote the minima of $L_{\mathrm{data}}$ restricted to the solution space (the hollow circle is a local minimum, the solid circle a global minimum).
}
\label{fig:disc_space_viz}
\end{figure}

The second approach (satisfying the ODE and evaluating the error in fitting the data) feels better-defined.
It is unclear that the extent-of-violation of an ODE is well-captured by the squared-error of the difference between the two sides of the equation.

However, there are pitfalls with this.
It may be difficult to find initial conditions, as discussed in section \ref{sec:sweep_and_shoot}.
Even if we were able to backpropogate through the ODE solver to compute the gradient of $L_{\mathrm{data}}$ with respect to our initial conditions, the loss function restricted to the solution-space may not be convex.
This is illustrated in figure \ref{fig:disc_space_viz}: if we start near the local minimum of the solution space, we will never find the global minimum by any method of small steps.

What I have therefore implemented is approach (1) followed by approach (2).

We begin by discretizing the space of functions on $[t_0, t_{N-1}]$ as a vector
$$
\left( \hat{x}(t_0), \hat{x}(t_1), \dots, \hat{x}(t_{N-1}) \right)
$$
which for notational convenience I will call
$$
\left( \hat{x}_0, \hat{x}_1, \dots, \hat{x}_{N-1} \right)
$$
from here on.
We require that these grid-points be evenly spaced, and denote the step-size as $h = t_i - t_{i-1}$.

Remember that we have measurements from only some of these times.
Let $\mathcal{I}$ denote the set of indices such that we have a measurement $\tilde{x}(t_i)$ at time $i$.
Again, for notational convenience I will call these measurements $\tilde{x}_i$ from here on.
These measurements may contain noise.

We now define two loss functions:%
\footnote{
We could instead use the backward derivative, $\hat{\dot{x}}(t_i) = h^{-1} \left( \hat{x}(t_{i}) -  \hat{x}(t_{i-1})\right)$. I do not think this should make a difference, but I have added an option to do so in my code.
}
\begin{align}
\label{eq:Ldata}
L_{\mathrm{data}} &= \frac{1}{2} \frac{1}{\left| \mathcal{I} \right|} \sum_{i \in \mathcal{I}} \left( \hat{x}_i - \tilde{x}_i \right)^2 \\
\label{eq:Lode}
L_{\mathrm{ODE}} &= \frac{1}{2} h \sum_{i=1}^{N-2} \left( \hat{\ddot{x}}_i - f\left( \hat{x}_i, \hat{\dot{x}}_i, t_i \right) \right)^2 \\
\mathrm{where} \, & \, \hat{\ddot{x}}_i = h^{-2} \left( \hat{x}_{i-1} - 2\hat{x}_i + \hat{x}_{i+1} \right) \\
\mathrm{and} \, & \, \hat{\dot{x}}_i = h^{-1} \left( \hat{x}_{i+1} - \hat{x}_i \right)
\end{align}

Note the range of the sum in \eqref{eq:Lode}: we could not extend this to $i=0$ or $N-1$, since we would not have the points $\hat{x}_{-1}$ and $\hat{x}_N$ needed to define the second derivative.
But this should not concern us.
Imagine that our task was to set $L_{\mathrm{ODE}}$ to zero.
Each term must vanish individually, so the sum gives us $N-2$ equations in $N$ unknowns ($\hat{x}_0, \dots, \hat{x}_{N-1}$).
For most choices of $f$ we will not be able to solve these equations, but for $N-2$ equations in $N$ unknowns we expect that, if a solution space exists, it will be 2 dimensional.
And that is exactly what we expect for a second-order ODE.

We begin by minimizing $L_{\mathrm{data}}$ (an easy task, since this is a convex loss function with constant curvature), and then minimize $L_\mathrm{ODE}$.
In practice we do this by gradient descent, minimizing
$$
L_{\mathrm{total}} = (1 - W_{\mathrm{ODE}}) L_{\mathrm{data}} + W_{\mathrm{ODE}} L_{\mathrm{ODE}}
$$
and varying $W_{\mathrm{ODE}}$ from close to zero at the start training to close to one at the end of training.
Schematically, we first proceed to the blue cross (exactly fitting the data) in figure \ref{fig:disc_space_viz}, and then to the green curve (the solution space).
Due to going via the blue cross, we should arrive at the solid yellow circle (the global minimum of $L_{\mathrm{data}}$ on the solution space).

%%FIXME
%and the derivatives by
%$$
%\hat{\ddot{x}}(t_i) = h^{-2} \left( \hat{x}(t_{i-1}) - 2 \hat{x}(t_i) + \hat{x}(t_{i+1}) \right)
%$$
%and%
%\footnote{
%We could instead use the backward derivative, $\hat{\dot{x}}(t_i) = h^{-1} \left( \hat{x}(t_{i}) -  \hat{x}(t_{i-1})\right)$. I do not think this should make a difference, but I have added an option to do so in my code.
%}
%$$
%\hat{\dot{x}}(t_i) = h^{-1} \left( \hat{x}(t_{i+1}) -  \hat{x}(t_i)\right)
%$$

\bibliographystyle{plain}
\bibliography{ode_collapser}

\end{document}


